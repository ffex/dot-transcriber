# Dot Configuration File
# Copy this file to config.toml and fill in your values

[telegram]
# Get this from @BotFather on Telegram
bot_token = "YOUR_BOT_TOKEN_HERE"
# Polling interval in seconds
poll_interval = 2

[transcription]
# Options: "whisper_local", "whisper_api" (not yet implemented)
service = "whisper_local"
# Language code (ISO 639-1)
language = "it"
# Model name (for reference only)
model = "base"
# Path to Whisper model file (download from: https://huggingface.co/ggerganov/whisper.cpp)
# Recommended for Italian: ggml-base.bin (142MB) or ggml-small.bin (466MB)
model_path = "./models/ggml-base.bin"

[ai_model]
# Options: "ollama_local", "ollama_remote", "anthropic" (future)
provider = "ollama_local"
# Model name - Recommended for Italian:
#   - llama3.2:3b (fast, good for M1 Mac)
#   - llama3.3 (best quality, needs powerful GPU)
#   - mistral (good balance)
#   - jobautomation/OpenEuroLLM-Italian (specialized for Italian)
model = "llama3.2:3b"
# API endpoint for Ollama
# Local: http://localhost:11434
# Remote (Windows PC in LAN): http://192.168.1.XXX:11434
endpoint = "http://localhost:11434"
# Temperature for generation (0.0 - 1.0)
# Higher = more creative, Lower = more focused
temperature = 0.7

[output]
# Directory where notes will be saved
notes_dir = "./output/notes"
# Directory where tasks will be saved (if enabled)
tasks_dir = "./output/tasks"
# Temporary directory for audio downloads
temp_dir = "./temp"

[features]
# Enable task extraction
enable_task_extraction = true
# Enable automatic tagging
enable_auto_tags = true
# Maximum audio file size in MB
max_audio_size_mb = 20

[logging]
# Log level: "error", "warn", "info", "debug", "trace"
level = "info"
# Log to file
log_file = "./dot.log"
